{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**노트북 1: 컴퓨터가 자연어를 이해하는 과정: 초기부터 현재까지**</font>\n",
    "\n",
    "### 시작: 컴퓨터와 자연어\n",
    "- 컴퓨터는 본래 숫자를 처리하는 기계입니다.  \n",
    "- 따라서 우리가 사용하는 자연어(한국어, 영어 등)를 처리하려면 **먼저 언어를 숫자로 바꾸는 과정**이 필요합니다.\n",
    "- 이처럼 **언어를 숫자로 바꾸고, 컴퓨터가 이해할 수 있도록 만드는 기술**을 자연어 처리(NLP, Natural Language Processing)라고 부릅니다.\n",
    "\n",
    "이번 노트북에서는  **초기의 단순한 표현 방식부터 최근의 고도화된 임베딩 방식까지**,  \n",
    "자연어를 숫자로 바꾸는 방법의 발전 과정을 쉽게 따라가 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기 단계: 희소 벡터(Sparse Vector)의 등장\n",
    "\n",
    "**희소 벡터란?**\n",
    "- 단어를 숫자로 바꾸는 가장 기본적인 방법입니다.\n",
    "- 각 단어마다 **고유한 위치를 가진 아주 긴 벡터**로 표현합니다.\n",
    "- 대부분의 값은 0이고, 해당 단어의 위치만 1로 표시합니다.\n",
    "\n",
    "이 방식을 **원-핫 인코딩(one-hot encoding)** 이라고 합니다.\n",
    "\n",
    "예시:\n",
    "- 어휘 사전: {\"나는\", \"고기를\", \"먹는다\", \"좋아해\"}\n",
    "- 원-핫 벡터:\n",
    "  | 단어     | 벡터          |\n",
    "  |----------|---------------|\n",
    "  | 나는     | [1, 0, 0, 0]  |\n",
    "  | 고기를   | [0, 1, 0, 0]  |\n",
    "  | 먹는다   | [0, 0, 1, 0]  |\n",
    "  | 좋아해   | [0, 0, 0, 1]  |\n",
    "\n",
    "### 문장 표현 방법: Bag of Words (BoW)\n",
    "\n",
    "- 문장을 구성하는 단어들의 **출현 여부만 기록하는 방식**입니다.\n",
    "- 예시 문장: \"나는 고기를 먹는다\"\n",
    "  - BoW 벡터: [1, 1, 1, 0]\n",
    "  - 의미: \"나는\", \"고기를\", \"먹는다\"는 문장에 포함됨(1), \"좋아해\"는 없음(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 벡터가 사용되었던 이유\n",
    "\n",
    "- **단순하고 계산이 쉬움**  \n",
    "  → 복잡한 연산 없이도 단어를 수치로 바꿀 수 있음\n",
    "- **당시 기술 수준에 적합**  \n",
    "  → 1990~2000년대에는 컴퓨터 성능이 낮았기 때문에 간단한 통계 기반 방법이 더 적합했음\n",
    "- **적용 분야**  \n",
    "  - 이메일 스팸 필터링\n",
    "  - 문서 유사도 비교\n",
    "  - 키워드 기반 검색 시스템\n",
    "\n",
    "### 희소 벡터의 한계\n",
    "1. **의미를 반영하지 못함**  \n",
    "   - 예: \"고기\"와 \"육류\"는 유사한 의미이지만 완전히 다른 벡터로 표현됨\n",
    "\n",
    "2. **차원의 저주(Dimensionality Curse)**  \n",
    "   - 어휘가 10만 개면 벡터도 10만 차원 → **메모리 낭비**와 **계산 비효율** 발생\n",
    "\n",
    "3. **문맥 정보가 없음**  \n",
    "   - 단어의 등장 여부만 기록하고, 그 **순서나 위치, 주변 단어** 등을 고려하지 않음\n",
    "\n",
    "4. **단어 순서의 중요성 무시**  \n",
    "   - 예:  \n",
    "     - \"dog bites man\" (개가 사람을 문다)  \n",
    "     - \"man bites dog\" (사람이 개를 문다)  \n",
    "   - 둘 다 BoW로 표현하면 [1, 1, 1] → **완전히 다른 문장인데 구분 불가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 발전: 밀집 벡터로의 전환\n",
    "\n",
    "---\n",
    "\n",
    "#### 밀집 벡터란?\n",
    "\n",
    "**희소 벡터(Sparse Vector) vs 밀집 벡터(Dense Vector)**\n",
    "\n",
    "- **희소 벡터**: 대부분 0으로 구성되어 있고, 특정 위치에만 1이 있는 형태 (예: 원-핫 인코딩)\n",
    "\n",
    "예시 1 (단어 \"사과\"):\n",
    "| 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "예시 2 (단어 \"배\"):\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "- **밀집 벡터**: 훈련된 Word2Vec 또는 임베딩 레이어의 결과로 나오는 연속된 실수값 벡터\n",
    "\n",
    "| 0.15 | 0.50 | 0.10 | 0.10 | -0.25 | 0.70 | -0.45 | 0.25 |\n",
    "|------|------|------|------|--------|------|--------|------|\n",
    "| 0.51 | 0.32 | 0.46 | 0.40 | -0.15  | 0.23 | -0.43  | 0.92 |\n",
    "\n",
    "\n",
    "#### 목적\n",
    "\n",
    "- 희소 벡터의 단점을 극복하여 단어의 의미를 더 잘 표현하고자 함\n",
    "- 단어를 짧은 실수 벡터로 표현  \n",
    "  - 예시: \"은행\" → [0.45, 0.1, -0.2] (보통 100~300차원)\n",
    "\n",
    "\n",
    "#### 대표 기술\n",
    "\n",
    "- Word2Vec (2013), GloVe, FastText 등\n",
    "\n",
    "\n",
    "#### 희소 벡터의 단점 극복 방식\n",
    "\n",
    "- **의미 학습**:\n",
    "  - 기존 희소 벡터는 단순 위치만 표시 (one-hot), 의미 정보 없음\n",
    "  - Word2Vec은 주변 단어(Co-occurrence)를 기반으로 의미 유사성 학습\n",
    "    - 예: \"은행\"과 \"금융\"은 유사한 벡터를 가짐\n",
    "    - 결과: 코사인 유사도 등으로 단어 간 유사도 계산 가능\n",
    "\n",
    "- **저차원 표현**:\n",
    "  - one-hot: 단어 수 만큼 차원 필요 (예: 10만 차원)\n",
    "  - 임베딩: 100~300차원 벡터로 압축 → 메모리, 연산 효율 증가\n",
    "\n",
    "- **문맥 기반 임베딩 (Word2Vec 한계와 BERT 등과의 차이점)**:\n",
    "  - Word2Vec은 고정된 벡터만 제공 (문맥 무시)\n",
    "  - 예: \"은행에 예금을 넣다\" vs \"강가의 은행에 앉다\" → Word2Vec은 동일 벡터\n",
    "  - BERT 등 문맥 기반 모델은 같은 단어라도 문맥에 따라 다른 벡터 제공\n",
    "\n",
    "\n",
    "\n",
    "#### 구현 기반\n",
    "\n",
    "- **딥러닝 활용**:\n",
    "  - Word2Vec: Skip-gram, CBOW 등 신경망 모델 사용\n",
    "  - 목적: 단어 간 의미적 관계 자동 학습\n",
    "\n",
    "- **대규모 데이터 활용**:\n",
    "  - 출처: 위키피디아, 뉴스, 웹 문서 등\n",
    "  - 활용: 방대한 문맥 데이터를 통해 단어 간 의미 관계 추출\n",
    "\n",
    "\n",
    "\n",
    "#### 성과 예시\n",
    "\n",
    "- 예시: \"king - man + woman ≈ queen\"\n",
    "  - 의미: 벡터 연산으로 성별, 직책 등의 관계를 수치적으로 표현 가능\n",
    "  - 참고: 모든 관계에서 완벽히 동작하는 것은 아니며, 경향성을 보이는 정도\n",
    "\n",
    "\n",
    "\n",
    "#### 임베딩 기술 비교\n",
    "\n",
    "| 모델      | 특징                                       |\n",
    "|-----------|--------------------------------------------|\n",
    "| Word2Vec  | 주변 단어 기반 학습 (CBOW, Skip-gram)     |\n",
    "| GloVe     | 전체 말뭉치 통계 기반 행렬 분해 방식       |\n",
    "| FastText  | Subword(단어 내부 단위) 기반 → 신조어 강함 |\n",
    "\n",
    "\n",
    "\n",
    "#### 임베딩 활용 예시\n",
    "\n",
    "- 문서 분류\n",
    "- 감정 분석\n",
    "- 기계 번역\n",
    "- 질의응답 시스템\n",
    "- 챗봇 및 검색 최적화\n",
    "\n",
    "\n",
    "\n",
    "#### 요약\n",
    "\n",
    "- 단어 임베딩은 단어를 **의미를 반영한 저차원 실수 벡터**로 표현하여, 기존 희소 표현의 비효율성과 의미 결여 문제를 효과적으로 해결함\n",
    "- 문맥 기반 표현까지 확장되면서 자연어 처리의 정밀도가 크게 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재까지의 흐름 요약\n",
    "- 초기:\n",
    "  - 방법: 희소 벡터로 단어 숫자화.\n",
    "  - 장점: 빠르고 단순.\n",
    "  - 단점: 의미 및 효율성 부족.\n",
    "- 현재:\n",
    "  - 방법: 밀집 벡터로 의미와 문맥 반영.\n",
    "  - 결과: 지능적인 NLP 발전.\n",
    "- 전망: 자연어 이해 방향으로 지속적 진화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 시대: 머신러닝과 딥러닝의 도약\n",
    "\n",
    "### 머신러닝이란?\n",
    "\n",
    "- **머신러닝(Machine Learning)** 은 컴퓨터가 **규칙을 명시적으로 프로그래밍하지 않아도**,  \n",
    "  **데이터를 통해 스스로 패턴을 학습**하는 기술입니다.\n",
    "- 쉽게 말하면, **사람이 일일이 \"이런 상황엔 이렇게 해!\"라고 가르치지 않아도**,  \n",
    "  컴퓨터가 많은 데이터를 보며 스스로 규칙을 찾아내는 방식입니다.\n",
    "\n",
    "### 초기 머신러닝: 통계 기반 자연어 처리\n",
    "\n",
    "- 자연어 처리는 초기에 **통계와 규칙에 기반한 방법**으로 시작되었습니다.\n",
    "- 예를 들어, 어떤 단어가 얼마나 자주 나오는지(**빈도**)를 기준으로 문장의 성격을 판단했습니다.\n",
    "\n",
    "### 예시: 스팸 이메일 분류\n",
    "\n",
    "- \"무료\", \"당첨\", \"지금 클릭\" 같은 단어가 많이 나오면 스팸일 가능성이 높다고 보고,  \n",
    "  **나이브 베이즈(Naive Bayes)** 같은 모델이 이를 수학적으로 계산하여 분류합니다.\n",
    "\n",
    "> 이처럼 머신러닝은 **단어의 출현 횟수**와 같은 간단한 통계 정보를 바탕으로 판단했습니다.\n",
    "\n",
    "<img src=\"imgs/sapm_classification.png\" width=500>\n",
    "\n",
    "\n",
    "### 통계 기반 머신러닝의 한계\n",
    "\n",
    "- 통계 정보만으로는 **문장 속 의미나 맥락을 잘 이해하지 못합니다.**\n",
    "\n",
    "예:\n",
    "- \"I like it\" → 긍정\n",
    "- \"I don’t like it\" → 부정\n",
    "\n",
    "→ 단어는 거의 같지만 **“don’t” 하나** 때문에 의미가 정반대입니다.  \n",
    "하지만 초기 모델은 단순히 단어 빈도만 보기 때문에 이 차이를 잘 구분하지 못했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 등장: 더 깊은 이해를 향해\n",
    "\n",
    "### 딥러닝이란?\n",
    "\n",
    "- **딥러닝(Deep Learning)** 은 머신러닝의 한 분야로,  \n",
    "  **신경망(Neural Network)** 을 여러 층으로 깊게 쌓은 모델입니다.\n",
    "- 마치 사람의 뇌처럼 정보를 단계별로 처리하며, **복잡한 패턴도 스스로 학습**할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "### 배경과 등장\n",
    "\n",
    "- **LSTM(Long Short-Term Memory)** 과 같은 순환신경망(RNN 기반 모델)은 **이미 1997년에 제안**되었지만,  \n",
    "  당시에는 **데이터 부족**과 **컴퓨팅 자원의 한계**로 널리 활용되지 못했습니다.\n",
    "\n",
    "- 이후 **2010년대**, GPU 발전과 대규모 데이터셋 확보가 가능해지면서  \n",
    "  딥러닝이 본격적으로 다양한 분야에서 성능을 발휘하기 시작했습니다.\n",
    "\n",
    "- 특히 이미지 인식(예: CNN)과 음성 인식 분야에서 **딥러닝 모델이 압도적인 성과**를 내면서,  \n",
    "  자연어 처리 분야에서도 **신경망을 활용하려는 움직임**이 본격화되었습니다.\n",
    "\n",
    "- 이러한 흐름 속에서 LSTM과 같은 모델도 **재조명**되며, 문장 구조와 문맥 이해에 활용되기 시작했습니다.\n",
    "\n",
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2015/03/genericDNN-624x568.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "## 왜 신경망이 필요한가?\n",
    "\n",
    "기존 머신러닝은 단어 자체는 볼 수 있었지만,  \n",
    "**단어 사이의 관계**, **순서**, **문맥 속 의미 변화**까지는 다룰 수 없었습니다.\n",
    "\n",
    "### 예시: 문맥에 따라 달라지는 단어\n",
    "\n",
    "- \"The bank is near the river.\"  \n",
    "- \"I deposited money in the bank.\"\n",
    "\n",
    "→ 두 문장 모두 \"bank\"가 들어있지만, 하나는 강둑이고 다른 하나는 금융기관입니다.  \n",
    "→ 이처럼 **문맥이 의미를 결정**하는 경우, 단순한 단어 빈도만으로는 분류가 어렵습니다.\n",
    "\n",
    "\n",
    "\n",
    "### 신경망의 강점\n",
    "\n",
    "- 신경망은 여러 층(layer)을 통해 입력 데이터를 점차 가공하면서 **복잡한 패턴을 추출**합니다.\n",
    "- 특히 **비선형 활성화 함수**를 사용하여 단순한 연산을 넘어서 **미묘한 관계나 의미 변화**도 포착할 수 있습니다.\n",
    "\n",
    "### 예시\n",
    "\n",
    "- \"I like it\" → 긍정  \n",
    "- \"I don’t like it\" → 부정  \n",
    "- \"I like it, but it’s expensive\" → 중립  \n",
    "\n",
    "→ 이처럼 단어 하나하나보다 **전체 문장 흐름과 조합**을 이해해야 정확한 판단이 가능합니다.  \n",
    "→ 신경망은 이런 **복잡한 의미 구조를 학습할 수 있는 도구**로 매우 적합합니다.\n",
    "\n",
    "\n",
    "\n",
    "## 정리\n",
    "\n",
    "- 머신러닝은 자연어를 처리하는 첫 번째 도약이었고,  \n",
    "  딥러닝은 그 한계를 뛰어넘어 **문맥과 의미까지 이해하는 길을 열었습니다.**\n",
    "- 딥러닝 기반의 자연어처리는 이후 **단어를 의미 있는 벡터로 표현하는 임베딩 기술**로 발전하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망(RNN): 시퀀스와 문맥의 첫걸음\n",
    "\n",
    "\n",
    "**왜 RNN이 필요했을까?**\n",
    "\n",
    "이전까지 신경망(예: CNN, MLP)은 주로 이미지처럼 **고정된 길이의 데이터**에만 잘 작동했습니다.  \n",
    "하지만 문장은 다음과 같은 특성이 있습니다:\n",
    "\n",
    "- 단어의 **순서가 중요**하고,  \n",
    "- **길이가 가변적**이며,  \n",
    "- 앞의 내용이 뒤에 영향을 주는 **문맥 의존성**이 강합니다.\n",
    "\n",
    "예를 들어,\n",
    "- “dog like you”와 “you like dog”는 어순에 따라 의미가 달라집니다.  \n",
    "- “그는 오늘 아침에…”에서 “아침에”가 나중 의미 결정에 영향을 줍니다.\n",
    "\n",
    "→ 이러한 **시퀀스(연속된 순서)** 데이터를 처리하기 위해 등장한 구조가 **RNN (Recurrent Neural Network)** 입니다.\n",
    "\n",
    "\n",
    "\n",
    "**순환신경망의 등장 RNN의 핵심 개념**\n",
    "\n",
    "RNN은 **이전 단어의 정보를 기억**하면서,  \n",
    "현재 단어를 처리할 수 있는 구조입니다.  \n",
    "즉, **시간에 따라 정보가 흐르도록** 설계된 신경망입니다.\n",
    "\n",
    "  ```\n",
    "      ┌───────────────┐\n",
    "      │  이전 상태 hₜ₋₁ |\n",
    "      └──────┬────────┘\n",
    "             │\n",
    "             ▼\n",
    "          ┌────────┐\n",
    "  xₜ ────►│   RNN   │────► hₜ\n",
    "          │   Cell │\n",
    "          └────────┘\n",
    "  ```\n",
    "- `xₜ`: 현재 입력 단어  \n",
    "- `hₜ`: 현재 상태(기억)  \n",
    "- `hₜ₋₁`: 이전 시점의 상태를 계속 전달하면서 정보가 흘러감\n",
    "\n",
    "**핵심 아이디어**: RNN은 이전 단어의 정보를 기억하며 다음 단어를 처리하는 구조로, 시간에 따라 데이터를 순차적으로 학습할 수 있었다.  \n",
    "**작동 방식**: \"나는 고기를 먹는다\"라는 문장에서 \"나는\"을 보고 \"고기를\"을, \"고기를\"을 보고 \"먹는다\"를 예측하며, 각 단계의 정보를 다음으로 전달했다.  \n",
    "**예시:**  \n",
    "문장: “나는 고기를 먹는다”\n",
    "\n",
    "- 1단계: \"나는\" 입력 → RNN이 `h₁` 생성  \n",
    "- 2단계: \"고기를\" 입력, `h₁`과 함께 처리 → `h₂` 생성  \n",
    "- 3단계: \"먹는다\" 입력, `h₂`와 함께 처리 → `h₃` 생성\n",
    "\n",
    "\t→ 이렇게 RNN은 **앞선 단어의 정보를 기억하며, 다음 단어를 예측하거나 이해**합니다.  \n",
    "\n",
    "\t<img src=\"https://blog.kakaocdn.net/dn/yOdSX/btqBOB5cHTF/az6aQR7W3sU7e4qDkaMwc1/img.gif\">\n",
    "\n",
    "**활용 예시**: RNN은 자연어 처리에서 다양한 실용적 문제를 해결하며 주목받았다.  \n",
    "- **텍스트 생성**: \"오늘은\" 입력 → \"날씨가\", \"좋다\" 등 다음 단어를 예측  \n",
    "- **기계 번역**: \"I eat meat\" → \"나는 고기를 먹는다\"  \n",
    "- **감정 분석**: \"이 영화는 정말 재미있다\" → 긍정 감정으로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq: 번역과 대화의 진화\n",
    "\n",
    "기존 RNN 구조만으로는 입력과 출력을 1:1로 연결하는 데 한계가 있었기 때문에,  \n",
    "**두 개의 RNN을 연결한 새로운 구조**가 등장합니다. 이것이 바로 **Seq2Seq (Sequence-to-Sequence)** 입니다.\n",
    "\n",
    "### 구조 설명\n",
    "\n",
    "<img src=\"https://dongju923.github.io/assets/images/NLP/seq2seq_4.gif\" width=700>\n",
    "\n",
    "- **인코더 RNN**: 입력 문장을 읽고, 하나의 **문맥 벡터(context vector)**로 압축  \n",
    "- **디코더 RNN**: 이 문맥 벡터를 바탕으로 **출력 문장을 생성**\n",
    "\n",
    "예:  \n",
    "- 입력: “I eat meat”  \n",
    "- 인코더 → 문맥 벡터 생성  \n",
    "- 디코더 → “나는 고기를 먹는다” 생성\n",
    "\n",
    "\n",
    "### Seq2Seq의 의미\n",
    "\n",
    "- 자연어 번역, 요약, 질문 생성 등 **문장을 문장으로 바꾸는 작업**에 매우 효과적이었습니다.\n",
    "- 특히 **Google Translate가 2016년부터 Seq2Seq 기반 번역**을 사용하면서  \n",
    "  번역 품질이 획기적으로 개선되었고, 대중적으로도 주목을 받게 되었습니다.\n",
    "\n",
    "\n",
    "### RNN과 Seq2Seq의 한계\n",
    "\n",
    "**1. 장기 의존성 문제 (Long-Term Dependency)**\n",
    "\n",
    "- 긴 문장에서 **초반 단어 정보를 끝까지 기억하기 어려움**\n",
    "\n",
    "예:\n",
    "- \"오늘 아침에 먹은 음식이 속이 안 좋아서…\"  \n",
    "- 뒤에 “속이 안 좋아서”가 나오더라도, “아침에 먹은 음식”이라는 정보가 희미해짐\n",
    "\n",
    "→ 이유: 정보가 RNN 안에서 계속 덮여 쓰이며 희석되기 때문\n",
    "\n",
    "\n",
    "**2. Seq2Seq의 정보 압축 한계**\n",
    "\n",
    "- 인코더가 **전체 문장을 단 하나의 벡터로 요약**하면서  \n",
    "  중요한 정보가 손실될 수 있음\n",
    "\n",
    "예:\n",
    "- \"The cat I saw yesterday was black\"  \n",
    "→ “yesterday” 같은 단어가 압축 벡터에 희미하게 담겨, 번역에서 빠질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망에서 어텐션으로: Seq2Seq의 진화\n",
    "\n",
    "\n",
    "##### Seq2Seq의 한계: 기억력 부족\n",
    "\n",
    "RNN 기반의 Seq2Seq 모델은 **문장을 하나의 벡터로 압축**해서 번역하거나 요약하는 구조였습니다.  \n",
    "하지만 이 방식에는 심각한 한계가 있었습니다:\n",
    "\n",
    "- 문장이 길어질수록 **초반 정보가 사라지거나 희미해지는 문제** 발생\n",
    "- 하나의 벡터로 모든 의미를 담기엔 **정보 손실**이 컸음\n",
    "\n",
    "##### 예시:\n",
    "\n",
    "- 입력 문장: \"The cat I saw yesterday was black\"\n",
    "- 번역 결과: \"검은 고양이입니다\" (❌ \"yesterday\"가 빠짐)\n",
    "\n",
    "→ 인코더가 “어제(yesterday)” 정보를 제대로 기억하지 못한 것입니다.\n",
    "\n",
    "\n",
    "##### 어텐션 메커니즘의 등장\n",
    "\n",
    "이런 문제를 해결하기 위해 **어텐션(attention)** 이라는 개념이 도입되었습니다.  \n",
    "어텐션은 이름처럼 **\"중요한 부분에 집중한다\"** 는 뜻입니다.\n",
    "\n",
    "<img src=\"imgs/attention.png\" width=500>\n",
    "\n",
    "\n",
    "##### 어텐션의 핵심 아이디어\n",
    "\n",
    "기존에는 전체 문장을 요약한 **하나의 벡터만** 디코더에 전달했지만,  \n",
    "어텐션은 **입력 문장의 모든 단어**를 **동시에 참고**합니다.\n",
    "\n",
    "- 디코더가 어떤 단어를 출력할 때,\n",
    "- 인코더의 모든 단어 중 **어떤 게 더 중요한지 가중치를 계산**\n",
    "- 중요한 단어에 더 많은 \"주의(attention)\"를 줌\n",
    "\n",
    "##### 예시:\n",
    "\n",
    "- 입력: \"나는 고기를 먹는다\"\n",
    "- 출력 중: \"eat\" (먹다)에 해당하는 단어를 만들 때\n",
    "- 디코더는 \"나는\"보다 **\"고기를\"에 더 집중** → 더 자연스럽고 정확한 결과 생성\n",
    "\n",
    "\n",
    "##### 어텐션의 효과\n",
    "\n",
    "- 긴 문장도 정확히 다룸\n",
    "- 특정 정보(yesterday, tomorrow 등)를 **잊지 않고 반영**\n",
    "- 자연스러운 번역·요약이 가능해짐\n",
    "\n",
    "> 2014~2015년 Google 번역 시스템에도 어텐션 기반 Seq2Seq가 도입되어  \n",
    "> 성능이 획기적으로 개선됨\n",
    "\n",
    "\n",
    "##### 어텐션의 확장 활용\n",
    "\n",
    "어텐션은 단순한 번역 외에도 다양한 작업에서 큰 효과를 보였습니다:\n",
    "\n",
    "- **질의응답**: “어제 본 고양이는 무슨 색이야?” → \"black\"에 집중  \n",
    "- **요약**: 긴 문서에서 핵심 문장에 주목해 요약 생성  \n",
    "- **감정 분석**: 문장 중 “하지만…” 이후의 전환 표현에 주목해 정확한 감정 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션의 진화: Transformer로의 전환\n",
    "\n",
    "\n",
    "##### 왜 LSTM 없이도 되는가?\n",
    "\n",
    "기존 RNN(LSTM 포함)은 **순차적으로 단어를 하나씩 처리**해야 했기 때문에,\n",
    "\n",
    "- **속도가 느리고**,  \n",
    "- **병렬 처리가 불가능**하며,  \n",
    "- 긴 문장에서 **문맥 유지가 어려움**\n",
    "\n",
    "하지만 어텐션은 **모든 단어 간 관계를 한 번에 계산**할 수 있었습니다.  \n",
    "→ “굳이 순차적으로 처리하지 않아도 된다!”는 가능성을 열게 된 것입니다.\n",
    "\n",
    "\n",
    "##### Transformer의 등장 (2017)\n",
    "\n",
    "2017년, Google의 논문 **Attention is All You Need**에서  \n",
    "**Transformer**라는 완전히 새로운 구조가 제안됩니다.\n",
    "\n",
    "- **RNN 없이**, 오로지 **어텐션만으로** 구성\n",
    "- 입력 전체를 동시에 바라보며 문맥을 처리\n",
    "- **인코더–디코더 구조**는 유지하되, 각 블록 내부는 모두 어텐션 기반\n",
    "\n",
    "  <img src=\"imgs/transformer.png\" width=500>\n",
    "\n",
    "##### 구조 개요:\n",
    "\n",
    "- **Self-Attention**: 문장 내 단어들끼리 서로 얼마나 관련 있는지 계산  \n",
    "- **Position Encoding**: 단어 순서 정보를 보완해줌 (순서를 무시하지 않도록)\n",
    "\n",
    "##### 예시:\n",
    "\n",
    "입력: “I eat meat”  \n",
    "출력: “나는 고기를 먹는다”  \n",
    "→ 모든 단어 관계를 동시에 고려하여 빠르고 정확하게 처리\n",
    "\n",
    "\n",
    "##### Transformer의 혁신\n",
    "\n",
    "- **속도 향상**: RNN처럼 단어를 하나씩 처리하지 않아도 되기 때문에,  \n",
    "  **GPU 병렬 처리**로 빠르게 학습 가능\n",
    "\n",
    "- **문맥 이해 향상**:  \n",
    "  \"The bank near the river\"와 같이 **동일한 단어(bank)**가 문맥에 따라  \n",
    "  다른 뜻을 가질 때도, 전체 문장을 보고 올바른 해석 가능\n",
    "\n",
    "\n",
    "##### Transformer 이후\n",
    "\n",
    "Transformer 구조는 이후 거의 모든 최신 NLP 모델의 기반이 되었습니다:\n",
    "\n",
    "| 모델 | 설명 |\n",
    "|------|------|\n",
    "| **BERT** | 문장의 문맥을 양방향으로 이해 (정확한 문장 분석에 강함) |\n",
    "| **GPT** | 다음 단어를 예측하며 문장을 생성 (대화·글쓰기 등에 강함) |\n",
    "| **T5 / BART** | 입력 → 출력 구조를 통합하여 다양한 작업에 활용 |\n",
    "\n",
    "\n",
    "##### 정리\n",
    "\n",
    "- RNN 기반 Seq2Seq는 혁신적이었지만, **기억력 한계**와 **속도 문제**가 있었음\n",
    "- 어텐션 메커니즘은 **중요한 단어에 집중**하면서 성능을 끌어올림\n",
    "- Transformer는 어텐션만으로 구성된 구조로  \n",
    "  RNN 없이도 더 빠르고 정밀한 NLP 시스템을 가능하게 만듦"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
